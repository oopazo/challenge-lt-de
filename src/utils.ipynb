{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11426a4a-cf47-495a-881f-90dc3ed7812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file reader resources\n",
    "from llama_index.readers.file import FlatReader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37a2f21-de01-41a5-bbd5-6914367752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser resouces\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd6a70-fcdf-4fec-b14e-1cf162c6db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector db resources\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41de9d9-ae45-426a-bd0d-357fe9177733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm resources\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28acfd9e-3e81-4a12-b229-f4213e49d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding resources\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8eddff-6e99-4c0a-b3d3-de34cbfb7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced metadata augmentation\n",
    "from llama_index.core.extractors import SummaryExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffcc95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom retriever resources\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e18217f-7603-4fa9-96bf-d825d8571f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up llm and enbedding models:\n",
    "Settings.llm = OpenAI(model=\"gpt-4.1-nano\")\n",
    "#Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7224cb4-c9cc-47aa-9171-dfde1c37be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"¿Cómo es el reajuste por IPC en Fakesoft? ¿Cómo es el reajuste por IPC en Ficticia?\",\n",
    "    \"¿Cuál es el cronograma para pagar establecido en el contrato con Fakesoft?\",\n",
    "    \"¿Qué tipo de almacenamiento utiliza Ficticia?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e156bd-d0fb-402d-af4c-584fd0b8ea01",
   "metadata": {},
   "source": [
    "#### reading files from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db7134c-9db6-4cca-89dc-30a75fc75c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files() -> list:\n",
    "    base_path = \"../data/\"\n",
    "    files = [\n",
    "        \"contrato_latam_fakesoft.md\", \n",
    "        \"contrato_latam_ficticia_anexo_b_especificaciones_tecnicas.md\", \n",
    "        \"contrato_latam_ficticia.md\"\n",
    "    ]\n",
    "    md_docs = []\n",
    "    for file in files:\n",
    "        rel_path = f\"{base_path}{file}\"\n",
    "        print(f\"reading file {rel_path}...\")\n",
    "        md_docs.extend(FlatReader().load_data(Path(rel_path)))\n",
    "    return md_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baa84c-e7a4-491b-8da0-82f72cd97baa",
   "metadata": {},
   "source": [
    "#### parsing / chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566ee7df-33f4-428b-b114-4abe92ade0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents: list) ->list:\n",
    "    \"\"\"since all documents are well-structured markdown, we can use the MarkdownNodeParser\n",
    "    to parse them into nodes\"\"\"\n",
    "    parser = MarkdownNodeParser()\n",
    "    return parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc57ab8-0ea8-48dd-a7b5-5001d1c950cf",
   "metadata": {},
   "source": [
    "#### vector db management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee1ca6e-b04f-4fed-8271-055c16f6a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_chroma_index(nodes: list, collection: str, mode: str = \"load\"):\n",
    "    persist_dir = \"../chroma_index\"\n",
    "    db = chromadb.PersistentClient(path=persist_dir)\n",
    "    chroma_collection = db.get_or_create_collection(collection)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = None\n",
    "    if mode == \"create\":\n",
    "        index = VectorStoreIndex(nodes=nodes, embed_model=Settings.embed_model, storage_context=storage_context)\n",
    "        print(f\"building vector index: {persist_dir}, collection: {collection}\")\n",
    "    elif mode ==  \"load\":\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store, storage_context=storage_context\n",
    "        )\n",
    "        print(f\"vector index loaded from storage: {persist_dir}, collection: {collection}\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838214a7-fbe4-43f6-94e6-a661de7e37ed",
   "metadata": {},
   "source": [
    "#### metadata management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44094ee-f350-4713-b35d-644f995d6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_node_structure(nodes: list) -> list:\n",
    "    \"\"\"By printing any node we can see that there is no clear separation betweeen what corresponds to metadata \n",
    "    and what corresponds to the text section extracted from our doc.\n",
    "    We can define a template to overwrite the structure of what's going to be send to the embedding model:\n",
    "    \"\"\"\n",
    "    updated_nodes  = []\n",
    "    for node in nodes:\n",
    "        # overwriting property:\n",
    "        node.text_template = \"METADATA:\\n{metadata_str}\\n---\\nCONTENT:\\n{content}\"\n",
    "        updated_nodes.append(node)\n",
    "    return updated_nodes\n",
    "\n",
    "def agument_metadata(nodes: list) -> list:\n",
    "    # the following operations could have been done in one shot. I'm splitting them into multiple \n",
    "    # separated functions just to be more explicit\n",
    "    nodes = company_augmentation(nodes)\n",
    "    nodes = exclude_metadata(nodes)\n",
    "    return nodes\n",
    "\n",
    "def company_augmentation(nodes: list) -> list:\n",
    "    # this information for source could be calculated or extracted from ingestion resources in a real-world\n",
    "    # pipeline, but for simplicity I'm just mapping it.\n",
    "    companies = {\n",
    "        'contrato_latam_fakesoft.md': 'Fakesoft',\n",
    "        'contrato_latam_ficticia_anexo_b_especificaciones_tecnicas.md': 'Ficticia',\n",
    "        'contrato_latam_ficticia.md': 'Ficticia'\n",
    "    }\n",
    "    for node in nodes:\n",
    "        filename = node.metadata[\"filename\"]\n",
    "        company = companies.get(filename)\n",
    "        node.metadata[\"company\"] = company\n",
    "    return nodes\n",
    "\n",
    "def exclude_metadata(nodes: list) -> list:\n",
    "    # extension is not useful at all for this experiment. We can discard it from metadata to avoid unnecessary noise\n",
    "    # and also fake relationships\n",
    "    for node in nodes:\n",
    "        if \"extension\" not in node.excluded_embed_metadata_keys:\n",
    "            node.excluded_embed_metadata_keys.append(\"extension\")\n",
    "        if \"extension\" not in node.excluded_llm_metadata_keys:\n",
    "            node.excluded_llm_metadata_keys.append(\"extension\")\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd190d-2aeb-42ac-9b2b-49e3d812c301",
   "metadata": {},
   "source": [
    "#### chunking & advanced metadata augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a736348-9e16-459e-aa59-23e12b8f7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_advanced_augmented_metadata(nodes: list) -> list:\n",
    "    prompt_template = \"\"\"\n",
    "    Here is the content of the section:\\n{context_str}\\n\\nSummarize the key topics and entities of the section using Spanish. \\n: \n",
    "    \"\"\"\n",
    "    sentence_splitter = SentenceSplitter(chunk_size=200, chunk_overlap=25, paragraph_separator=\"\\n\\n\")\n",
    "    summary_extractor = SummaryExtractor(llm=Settings.llm, prompt_template=prompt_template)\n",
    "    pipeline = IngestionPipeline(transformations=[\n",
    "        sentence_splitter,\n",
    "        summary_extractor,\n",
    "    ])\n",
    "    nodes = pipeline.run(nodes=nodes, show_progress=True)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1dbdb-c348-41f2-b650-ef38f3d76551",
   "metadata": {},
   "source": [
    "#### retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a414873-b98b-4bf5-9844-6904c218186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index: VectorStoreIndex, query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    I've added an additional parameter to be able to switch vector indexes just for demo purposes\n",
    "    \"\"\"\n",
    "    query_engine = index.as_query_engine(\n",
    "        model=Settings.llm, \n",
    "        # Adding metadata filters can be a great idea when multiple companies are involved in the same index. \n",
    "        # Just letting it commented out for now...\n",
    "        # filters=MetadataFilters(\n",
    "        # filters=[\n",
    "        #     MetadataFilter(key=\"company\", value=\"Ficticia\", operator=FilterOperator.EQ),\n",
    "        # ]\n",
    "        # ),\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    response_dict = {\"answer\": None, \"relevant_nodes\": None}\n",
    "    if response:\n",
    "        relevant_nodes = [r for r in response.source_nodes]\n",
    "        response_dict[\"answer\"] = response\n",
    "        response_dict[\"relevant_nodes\"] = relevant_nodes\n",
    "    return response_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed56827",
   "metadata": {},
   "source": [
    "#### Improved retireval function with custom retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0202c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_retriever(index: VectorStoreIndex, query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Here is a demo that shows how to implement a retriever step-by-step. It brings more granular control.\n",
    "    \"\"\"\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        # Adding metadata filters can be a great idea when multiple companies are involved in the same index. \n",
    "        # Just letting it commented out for now...\n",
    "        # filters=MetadataFilters(\n",
    "        # filters=[\n",
    "        #     MetadataFilter(key=\"company\", value=\"Ficticia\", operator=FilterOperator.EQ),\n",
    "        # ]\n",
    "        # ),\n",
    "        similarity_top_k=5,\n",
    "    )\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        llm=Settings.llm,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    response_dict = {\"answer\": None, \"relevant_nodes\": None}\n",
    "    if response:\n",
    "        relevant_nodes = [r for r in response.source_nodes]\n",
    "        response_dict[\"answer\"] = response\n",
    "        response_dict[\"relevant_nodes\"] = relevant_nodes\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d814d-8505-499d-9c53-f61380022156",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2689472b-2c12-40ed-b354-643ec020b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node(node) -> None:\n",
    "    print(node.get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c29dbe",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b427ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_queries(index: VectorStoreIndex, queries: list, custom_retr: bool = False) -> None:\n",
    "    \"\"\"Main entrypoint. Calls the retrieve function for each query and prints the results.\"\"\"\n",
    "    for query in queries:\n",
    "        print(\"==================================================\")\n",
    "        print(f\"query: {query}\")\n",
    "        print(\"==================================================\")\n",
    "        if custom_retr:\n",
    "            response = custom_retriever(index=index, query=query)\n",
    "        else:\n",
    "            response = retrieve(index=index, query=query)\n",
    "        print(\"answer:\")\n",
    "        print(response[\"answer\"])\n",
    "        print(\"==================================================\")\n",
    "        print(\"relevant_nodes:\")\n",
    "        for r in response[\"relevant_nodes\"]:\n",
    "            print_node(r)\n",
    "        print(\"==================================================\")\n",
    "        print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
